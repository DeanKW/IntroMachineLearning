\input{../note_template}
\lhead{github.com/DeanKW/IntroMachineLearning}
\rhead{Chapter 3: Linear Regression}
\chead{\textbf{ISLP Notes}}
\title{Chapter 3 Summary: Linear Regression}
\begin{document}
    \maketitle

        Linear regression (yes, the one from middle or high school) may seem boring and not worth your time, but it is! 
        It is widely used and many    fancier models are generalizations or extensions of linear regression.    So seriously, this is important.
        \section*{3.1 Simple Linear Regression}
        \textbf{Simple Linear Regression} assumes that the response variable, \(Y\) has a linear relationship to a single predictor variable \(X\).
        \begin{equation*}\tag{3.1}
            Y \approx \beta_0 + \beta_1 X
        \end{equation*}
        
        
        \begin{note}
            This is the same relationship as \(y=mx+b\) you likely remember from school.\\
            \(\beta_0\) represents the intercept, \(b\) while \(\beta_1\) represents the slope, \(m\).
        \end{note}
        
        \(\beta_0\) and \(\beta_1\) are referred to as the \textbf{coefficients} or \textbf{parameters}.\\
        We will use our training data to estimate these parameters and then indicate and reprsent our \textbf{model} as
        \begin{equation*}\tag{3.2}
            \hat y = \hat \beta_0 + \hat \beta_1 X
        \end{equation*}

        \begin{note}
            We call this ``hat notation.'' In statistics, a hat over something indicates it's either an
            \href{https://en.wikipedia.org/wiki/Estimator}{estimator} or an estimated value.
        \end{note}
        
        \subsection*{3.1.1  Estimating the Coefficients}\label{estimating-the-coefficients}
        
        Since we do not know the true relationship between \(X\) and \(Y\), we use the training data to estimate \(\beta_0\) and \(\beta_1\)

        \begin{note}
            Just like you did in middle/high school when calculating the line of best fit
        \end{note}
        
        We will use \textbf{least squares} as the criteria to determine what values to use for our parameters, \(\beta_0\) and \(\beta_1\).
        
        The \(i\)th \textbf{residual} is the difference between the \(i\)th response variable and our prediction for that variable
        \begin{equation*}
        e_i = y_i - \hat y_i
        \end{equation*}
        
        The \textbf{residual sum of squares} is defined
        
        
        \begin{align*}
            \text{RSS} &= e_1^2 + e_2^2 + \cdots + e_n^2 \\
            &= (y_1 - \hat y_1)^2 +  (y_2 - \hat y_2)^2 + \cdots +  (y_n- \hat y_n)^2 \\
            &= (y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + (y_2 - \hat \beta_0 - \hat \beta_1 x_2)^2  + \cdots + (y_n - \hat \beta_0 - \hat \beta_1 x_n)^2 
        \end{align*}
        
        
        The parameters can be calculated as:
        
        \begin{align*}\tag{3.4}
        \hat \beta_1 &= \frac{\sum_{i=1}^n(x_i - \bar x)(y_i-\bar y)}{\sum_{i=1}^n(x_i - \bar xx)^2}\\
        \hat \beta_0 &= \bar y - \hat \beta_1 \bar x
    \end{align*}
     where \(\bar y\) and \(\bar x\) are the sample mean, defined below
     \begin{equation*}
           \bar y = \frac{1}{n}\sum_{i=1}^n y_i \qquad \text{and} \qquad \bar x = \frac{1}{n}\sum_{i=1}^n x_i
     \end{equation*}

        \subsection*{3.1.2 Assessing the Accuracy of the Coefficient Estimates}
        \paragraph{Standard Error\\}
        \textbf{Standard error} tells us the average amount that an estimate differs from the actual value.
        
        \begin{equation*} \tag{3.7}
            \text{Var}(\hat \mu) = \text{SE}(\hat \mu)^2 = \frac{\sigma^2}{n}
        \end{equation*}
        
        
        \begin{equation*} \tag{3.8}
            \text{SE}(\hat \beta_0)^2 = \sigma^2 \left[ \frac{1}{n} + \frac{\bar x ^2}{\sum_{i=1}^n (x_i - \bar x)^2} \right]
        \end{equation*}
        
        \begin{equation*} \tag{also 3.8}
            \text{SE}(\hat \beta_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2}
        \end{equation*}
        
        where \(\sigma^2 = \text{Var}(\epsilon)\) While we generally don't know \(\sigma^2\), it can be estimated from the data.\\
        The estimate of \(\sigma\) is called \textbf{residual standard error}
        \begin{equation*}
            \sigma = \text{RSE} = \sqrt{\frac{\text{RSS}}{n-2}}
        \end{equation*}
        
        \paragraph{Confidence Intervals\\}
         Standard errors can be used to compute    \textbf{confidence intervals}. The 95\% confidence interval for
        \(\beta_0\) and \(\beta_1\) are approximately: 
        \begin{gather*}
            \hat \beta_0 \pm 2 \cdot \text{SE}(\hat \beta_0) \tag{3.11}\\
            \hat \beta_1 \pm 2 \cdot \text{SE}(\hat \beta_1)
        \end{gather*}
        
        This means there is approximately a 95\% chance that the interval
        \begin{equation*} \tag{almost 3.10}
            \left[\beta_0 - 2 \cdot \text{SE}(\hat \beta_0), \beta_0 + 2 \cdot \text{SE}(\hat \beta_0)\right]            
        \end{equation*}
        
        contains the true value for \(\beta_0\). 
        \textbf{NOTE: This is an incorrect interpretation of a confidence interval, but it is what the book writes.  Please see the warning below}
        
        \begin{note}
            1.96 is closer to the correct value than 2. This value comes from the Z-score value for a 97.5\% quantile of a t-distribution. You
            are likely to see these in any statistics class.
        \end{note}
        
        \begin{warning}
            A 95\% confidence interval does \textbf{NOT} mean we there is a 95\% chance that the true parameter lies within the range.\\
            What it really means is that if we sampled the data 100 times, each time calculating the parameter and confidence interval, 95\% of those
            confidence intervals would contain the true value of the parameter.\\
            It's a small distinction, but I wanted to make it, even if the book did not.\\
            We generally say that we are 95\% confident that the true parameter lies in the range, not that the probability is 0.95.
        \end{warning}
        
        \paragraph{Hypothesis Testing\\}
        Standard errors can also be used to perform hypothesis testing.  The most common of which is the \emph{null hypothesis}.  We will test to see if
        the data provides evidence to reject the null hypothesis (that two variables/phenomena/results have no relationship) in favor of the alternative hypothesis (that there is a relationship)
        \begin{equation*}\tag{3.12}
            \underbrace{H_0}_{\text{Null hypothesis}}: \text{There is no relationship between } X \text{ and } Y \Longrightarrow \beta_1=0
         \end{equation*}

         \begin{equation*}\tag{3.13}
            \underbrace{H_1}_{\text{Alternative hypothesis}}: \text{There is a relationship between } X \text{ and } Y\Longrightarrow \beta_1 \neq 0
         \end{equation*}
         We compute the \textbf{t-statistic}:
         \begin{equation*}\tag{3.14}
             t = \frac{\hat \beta_1-0}{\text{SE}(\hat \beta_1)}
          \end{equation*}
          If $\beta_1 = 0$, this will have a $t$-distribution with $n-2$ degrees of freedom.  The probability of observing a value greater than or equal to $|t|$ is called a \textbf{p-value} .  If the p-value is small, there it is unlikely that the relationship between predictor and response is due to chance.
         We \textbf{reject} the null hypothesis (and accept the alternative hypothesis, if the p-value is "small enough")
         
         \begin{interesting}
             Did you know that the $t$-test was developed by in order to make better beer?  William Sealy Gosset, while the head experimental brewer at Guinness, developed the $t$-test as a way to study the quality of the barley used in brewing Guinness.
         \end{interesting}

        \subsection*{3.1.3 Assessing the Accuracy of the Model}
        \paragraph{Residual Standard Error\\}
        Suppose we knew the exact true model, $Y\approx \beta_0+\beta_1 X$ (from equation 3.1), recall that there is the irreducible error, $\epsilon$, associated with every term.  Residual Standard Error (RSE) will attempt to estimate the standard deviation of that irreducible error.\\
        
        It does this by seeing how well your model fits the data!  Convenient, let's calculate the standard deviation of our residuals!  
        
        \begin{equation*}\tag{3.15}
            \text{RSE} = \sqrt{\frac{1}{n-2}\text{RSS}} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i - \hat y_i)^2}
        \end{equation*}
        RSE tells us about the spread of the observed values from the predicted ones, how far off the model's predictions are, on average..  A lower RSE means the predicted values are closer to the observed ones.\\
        
        RSE is in whatever units the $Y$ variable is in, so it can be hard to understand what it really means.   For that, let's look at...
        \paragraph{$R^2$ Statistic\\}
        $R^2$ also measures the accuracy of the model, but does it as a proportion so the values are always between 0 and 1.
        \begin{equation*}\tag{3.17}
            R^2 = \frac{\text{TSS}-\text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}
        \end{equation*}
        where TSS, the \textbf{total sum of squares} is defined
        \begin{equation*}
            \text{TSS} = \sum_{i=1}^n(y_i - \bar y)^2
        \end{equation*}
        \begin{itemize}
            \tightlist
            \item TSS is the total variance in the response, $Y$.  Think of it as the amount of variability in the response (not dependent on the model)
            \item RSS measures the amount of variability that is left unexplained after performing the regression
            \item TSS - RSS then measures the amount of variability in the response that is explained by performing the regression.
            \item $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$
        \end{itemize}
        An $R^2$ value of $1$ means the model perfectly explains all variability.  An $R^2$ of $0$ means the model explains none of it.  It does just as well as predicting the mean.
        
        \textbf{Should I add correlation?}
        
        \section*{3.2 Multiple Linear  Regression}\label{multiple-linear-regression}
        What happens if we have more than one dependent variable?  If we have $p$ input variables, a multiple linear regression model takes the form:
        \begin{equation*}\tag{3.19}
            Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
         \end{equation*}
         $X_j$ is the $j$th predictor, $\beta_j$ is the corresponding coefficient.  It can be interpreted as the average effect on $Y$ of a one unit increase in $X_j$, given that no other predictor variables change.
        
        *{3.2.1 Estimating the Regression Coefficients}\label{estimating-the-regression-coefficients}
        Our multiple linear regression model will be
        \begin{equation*}\tag{3.21}
            \hat y = \hat \beta_0 + \hat \beta_1 x_1 + \hat \beta_2 x_2 + \cdots + \hat \beta_p x_p.
        \end{equation*}
        Like in simple linear regression, we will choose our $\beta$s in order to minimize the residual sum of squares.
        \begin{align*}
            \text{RSS} &= \sum_{i=1}^n (y_i - \hat y_i)^2\\
                                &= \sum_{i=1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_{i1} - \hat \beta_2 x_{i2} - \cdots - \hat \beta_p x_{ip})^2
        \end{align*}
        You'll never compute the coefficients by hand, so we omit the actual calculation.  It's fine to know that they are chosen to minimize RSS.
        \begin{note}
            While not covered by ISLP, in order to make the book more approachable (and not dependent on linear algebra), you will often see multiple linear regression represented in matrix form.
            If we let
            \begin{equation*}
                \boldsymbol{\beta} = \begin{bmatrix*}
                    \hat \beta_0\\
                    \hat \beta_1 \\
                    \hat \beta_2 \\
                    \vdots \\
                    \hat \beta_n
                \end{bmatrix*} \qquad \text{and} \qquad 
                \mathbf{X}_i= \begin{bmatrix*}
                    1 & x_{i1} & x_{i2} & \cdots & x_{in}
                \end{bmatrix*}
            \end{equation*}
            then
            \begin{equation*}
                \hat y_i = \mathbf{X}_i\boldsymbol{\beta}
             \end{equation*}
             
             Note that you will likely see this represented as multiple rows in $\mathbf{X}$ and $\mathbf{Y}$ representing different observations.  I've omitted that to make it easier to understand.
        \end{note}
        
        \begin{note}[Interpreting Regression Coefficients]
           \begin{itemize}
               \item Ideally, want predictors to be uncorrelated (though rarely happens)
               \begin{itemize}
                   \item Lets each coefficient be estimated and tested separately
                   \item Can make interpretations such as "a unit change in $X_j$ is associated with a $\beta_j$ change in $Y$, while all the other variables stay fixed
               \end{itemize}
              \item Correlations amongst predictors cause problems
              \begin{itemize}
                  \item Variance of all coefficients tends to increase
                  \item  Interpretations becomes difficult, when $X_j$ changes, so too do others
                  \begin{itemize}
                      \item Suppose a company has a fixed advertising budget, if you increase TV marketing, you must decrease internet marketing.
                  \end{itemize}
              \end{itemize}
             \item Claims of causality should be avoided for observational data
             \item It's possible (and likely) that a predictor that might have a big effect when the sole predictor has little to no effect, if it's correlated with another predictor.
           \end{itemize}
          \end{note}

        \subsection*{3.2.2 Some Important Questions}\label{some-important-questions}
        \begin{enumerate}
            \item Is at least one of the predictors $X_1, X_2, \cdots, X_p$ useful in predicting the response?
            \item Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?
            \item How well does the model fit the data?
            \item Given a set of predictor values, what response value should we predict, and how accurate is our prediction?
        \end{enumerate}
         \subsubsection*{Is there a Relationship between the Response and Predictors?}
         We use the F-Statistic
         \begin{equation*}\tag{3.23}
             F = \frac{\text{TSS}-\text{RSS}/p}{\text{RSS}/(n-p-1)}
          \end{equation*}
          \begin{itemize}
          	\item Tests that all coefficients are $0$.
              \item When there is no relationship between the response and predictors, the $F$-statistics takes on a value close to 1.  If there is a relationship(and $H_a$ is true), the $F$ statistic is greater than 1.
              \item How large the $F$-statistic must be depends on the values of $n$ and $p$.
              \item For large $n$, smaller $F$-statistic can provide evidence against $H_0$.  If $n$ is small, need a larger $F$-statistic.
          \end{itemize}
          If the null hypothesis is that a specific subset of coefficients are zero (for convenience, the ones testing are at the end of the list):
          \begin{equation*}
              H_0: \beta_{p-q+1} = \beta_{p-q+2} = \cdots = \beta_{p} = 0
           \end{equation*}
           we fit a second model that uses all the variables \textbf{except} the subset we are testing.  Call $\text{RSS}_0$ the residual sum of squares of this second model.  then the $F$-statistic is
           \begin{equation*}\tag{3.24}
                             F = \frac{\text{RSS}_0-\text{RSS}/q}{\text{RSS}/(n-p-1)}
           \end{equation*}

			\begin{itemize}
				\item  The $p$-values of individual regressors (which provided information on if that variable was related to the response ), is equivalent to the $F$-statistic omitting that single variable from the model, leaving the rest in.
					\begin{warning}[Why bother with F statistics?]
					Suppose we have 100 predictors $p=100$ and none are related to the response, meaning $H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0$ is true.  There is a 5\% chance that any given $p$-value is below 0.05, totally by chance!  So we would expect 5 of them to have small $p$-values, despite there being no true association between the predictor and response!  This means, \textbf{If we use the individual $t$-statistics and associated $p$-values in order to decide whether or not there is any association between the variables and the response, there is a very high chance that we will incorrectly conclude that there is a relationship}.  The $F$-statistic does not suffer this problem because it adjusts for the number of predictors.  If $H_0$ is true (and there is no association between any predictor and the response), there is only a 5\% chance that the $F$-statistic will result in a $p$ value below 0.05.
				\end{warning}
				\item  Using an $F$-statistic to test for any association works when $p$ is relatively small compared to $n$.
				\begin{itemize}
					\item  If $p > n$, then there are more coefficients $\beta_j$ to estimate than observations from which to estimate.  Then, we cannot use least-squares to fit the multiple linear regression, so the $F$-statistic cannot be used 
					\item   When $p$ is large, some approaches discussed soon, like forward selection can be used.  More detail about handling high-dimensional settings is in Chapter 6.
				\end{itemize}
			\end{itemize}
		
            

           

         \subsubsection*{Deciding on Important Variables}
         \begin{itemize}
             \item Most direct approach is called \textbf{all subsets} or \textbf{best subsets} regression.  Compute the least squares fit for all possible subsets and then choose between them based on some criterion that balances training error with model size.
             \item Can't do when large number of regressors, since there are $2^p$ possible subsets.  Even $p=40$ results in over a billion models!
         \end{itemize}
         \paragraph{Three Main Approaches}
         \begin{itemize}
         	\item \textbf{Forward Selection} - Begin with \textbf{null model} (has intercept, but no predictors).  Then fit $p$ simple linear regressions and add to the null model, the variable that results in the lowest RSS.  Repeat for the new one-variable, two-variable, etc model.
         	\begin{itemize}
         		\item Greedy approach - might include variables early on that later become redundant
         	\end{itemize}
         	\item \textbf{Backward Selection} - start with all variables in model, remove the variable with largest $p$ value.  Repeat process
         	\begin{itemize}
         		\item Cannot be used if $p > n$
         	\end{itemize}
         	\item \textbf{Mixed Selection} - Start with  null model, add variables one-by-one.  If as add new variables, the $p$-value for a model reaches a threshold, remove it.  Continue going foward and backwards until all variables have low $p$ value and all variables not in the model have large $p$-value.         \end{itemize}

         \subsubsection*{Model Fit}
         \begin{itemize}
         	\item \textbf{RSE}
         	\item $R^2$ - Recall, in simple linear regression, is correlation of response and variable.  In multiple, $\operatorname{Cor}(Y, \hat Y)^2$
         	\begin{itemize}
         		\item Note: Property of fitted linear model is maximizes the correlation among all possibel linear models
         		\item $R^2$ close to $1$ means model explains a large portion of variance in the response variable.
         		\item $R^2$ always increases as more variables are added to the model, even if those variables only weakly associated with response, since get better fit to training data.  Small increase probably means omit variable.
         	\end{itemize}
         	
         \end{itemize}
         end{itemize}
         \subsubsection*{Predictions}
        
        \section*{3.3 Other Considerations in the Regression Model}\label{other-considerations-in-the-regression-model}
        
        \section*{3.4 The Marketing Plan}\label{the-marketing-plan}
        
        \section*{3.5 Comparison of Linear Regression with K-Nearest Neighbors}\label{comparison-of-linear-regression-with-k-nearest-neighbors}
        
    
\end{document}