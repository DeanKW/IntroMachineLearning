\input{../note_template}
\begin{document}
\section*{Chapter 2 Summary: Statistical Learning}\label{chapter-2-summary-statistical-learning}

\subsection*{2.1: What is Statistical Learning?}\label{what-is-statistical-learning}

\textbf{Statistical Learning} is a set of tools for estimating a function, \(f\), that connects input variables 
\(X\) and output variables \(Y\). We estimate \(f\) because we do not (and often can not)
know the true relationship. \\

\noindent \textbf{Goal:} Estimate the function \(f\) that relates \(X\) and \(Y\):
\begin{equation*}
    Y = f(X) + \epsilon
\end{equation*}
    Given data \((X_1, Y_1), (X_2, Y_2), ..., (X_n, Y_n)\), where:
\begin{itemize}
    \tightlist
    \item \(X\): Predictors/Independent Variables/features
    \item \(Y\): Response/Dependent Variable
    \item  \(\epsilon\): Irreducible error - measurement errors and other discrepancies
\end{itemize}

\subsubsection*{\texorpdfstring{2.1.1: Why Estimate \(f\)?}{2.1.1: Why Estimate f?}}\label{why-estimate-f}


Two primary reasons to estimate \(f\): \textbf{prediction} and \textbf{inference}

\textbf{Prediction} - Using input variables \(X\) to predict an output variable \(Y\)

\begin{equation*}
    hat Y = \hat f (X)
\end{equation*}
\begin{itemize}
\tightlist
\item \(\hat Y\): Estimate for \(Y\)
\item \(\hat f\): Estimate of the true function \(f\) 
\item \(X\): Predictor variables
\end{itemize}

We can represent the error by the average, or \emph{expected value} of
the squared difference between the predicted value and the actual value.
This will be explained more in the future. We call this the
\emph{Expected Prediction Error (EPE)} Just note that this consists of
\textbf{reducible} and an \textbf{irreducible} portion.

\begin{align*}
E(Y-\hat Y)^2 &= E[f(X) + \epsilon + \hat f(X)]^2 \\
&= [f(X)-\hat f(X)]^2 + \text{Var}(\epsilon)\\
&= \text{reducible error} + \text{irreducible error}
\end{align*}

\begin{note}
    \(E\) means expected value, NOT error
\end{note}

\begin{itemize}
\tightlist
\item
  \textbf{Reducible} error - The error in our model

  \begin{itemize}
  \tightlist
  \item
    \([f(X)-\hat f(X)]^2\)
  \end{itemize}
\item
  \textbf{Irreducible} error - The error we cannot get rid of.

  \begin{itemize}
  \tightlist
  \item
    Even if we knew \(f(x)\), we would still make errors in prediction,     since for any \(X=x\), there is a distribution of possibly \(Y\) values
  \item  \(\text{Var}(\epsilon)\)
  \end{itemize}
\end{itemize}

\textbf{Inference} - Understanding the relationship between \(X\) and
\(Y\).

\subsubsection*{\texorpdfstring{2.1.2: How do we estimate
\(f\)?}{2.1.2: How do we estimate f?}}\label{how-do-we-estimate-f}

\paragraph*{Parametric vs.~Non-Parametric Models}\label{parametric-vs.-non-parametric-models}

\subparagraph{Parametric}\label{parametric}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume some functional form for \(f\). This is choosing the model. One
  example would be linear:
  \[f(x) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p\] 
  \item
  Fit or train the model. Use some procedure to estimate the parameters
  (in this case \(\beta_0, \beta_1, \cdots, \beta_p\)). That is to say,
  find values for the parameters such that
  \[Y \approx  \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p\]
\end{enumerate}


In this example, we have simplified the problem of estimating \(f\) down to fitting parameters \(\beta_0, \beta_1, \cdots, \beta_p\)
\begin{itemize}
    \tightlist
    \item Too flexible a model leads to underfitting and poor predictions
    \item Too complex a model leads to overfitting
    \item Choosing a functional form that is very different from the true form of \(f\) will result in poor results
\end{itemize}

\subparagraph{Non-Parametric}
\begin{itemize}
    \tightlist
    \item No explicit assumption about the    functional form of \(f\)
    \item More flexible since no specific function shape, can fit many shapes
    \item Requires far more training data to     accurately estimate \(f\) since the problem isn't reduced to estimating    parameters 
    \item Easy to overfit to training data
\end{itemize}

\paragraph{Supervised vs.~Unsupervised Learning}\label{supervised-vs.-unsupervised-learning}

\subparagraph{Supervised}

\begin{itemize}
\tightlist
\item  For each observation, we have the input measurements, \(x_i\) and the  output, \(y_i\) is labeled
\item  Ex: Linear Regression, Classification
  \end{itemize}
  
   \subparagraph{Unsupervised}
   \begin{itemize}
       \tightlist
\item  For each observation, we have the input measurements \(x_i\), but  there is no associated response, \(y_i\)
\item  Often seek to understand the relationship between variables or  observations
\item  Ex: Clustering
\end{itemize}

\paragraph{Regression vs.~Classification Problems}\label{regression-vs.-classification-problems}

\subparagraph{Regression}\label{regression}

\begin{itemize}
\tightlist
\item  The response (\(Y\)) is quantitative (a number)
\item  Ex: Predicting income or the value of a house 
\end{itemize}
\subparagraph{Classification}
\begin{itemize}
\tightlist
\item  The response (\(Y\)) is qualitative (a category or label)
\item  Attempting to predict what group something is in
\item  Ex: Is an email spam or not spam?
\end{itemize}

\subsection*{2.2: Assessing Model Accuracy}\label{assessing-model-accuracy}

\subsubsection*{2.2.1 Measuring the Quality of Fit}\label{measuring-the-quality-of-fit}

When estimating the function \(\hat f (x)\) we need to determine how well it predicts the response variable, \(Y\).
The most common way of doing this is with the \textbf{Mean Squared Error (MSE)}.

\begin{equation*}
    \text{MSE} = \frac{1}{n}\sum_{i=1}^{n}\left(y_i - \hat f(x_i) \right)^2
\end{equation*}
\begin{itemize}
    \tightlist
    \item \(y_i\) - actual value
    \item  \(\hat{f}(x_i)\) - predicted value
    \item \(n\) - number of data points
\end{itemize}

If MSE were computed using the training data, we would fit our training points as closely as possible and would surely overfit, so we don't really care how \(\text{MSE}_{\text{train}}\) does! We are far more interested in how our model predicts new, unseen data! So what we want to do is split our test data and training data, then test them separately!

\textbf{Test error is what we really care about, not the training error}

\begin{itemize}
\tightlist
\item  Flexible (complex) models fit the training data very well and thus  have low training error
\item   They may not generalize well to the test data, which results in poor  results. We call this \textbf{overfitting}
\end{itemize}
\subsubsection*{2.2.2: The  Bias-Variance Trade-Off}
\begin{itemize}
\item  \textbf{Bias} - the error from wrong assumptions in the model

  \begin{itemize}
  \tightlist
  \item  Can think of this as error caused by simplifying a real-life problem  into a much simpler model
  \end{itemize}
\item  \textbf{Variance} - error from the model being sensitive to small changes in the training set

  \begin{itemize}
  \tightlist
  \item    Can think of this as ``how much would \(\hat f\) change if trained  on different data"
  \end{itemize}
\item  High bias - underfit (model too simple)
\item  High variance - overfit (model too complex)
\end{itemize}

\begin{align*}
\text{E}[\text{MSE}]&=\text{Variance}+ \text{Bias}^2 + \text{Irreducible Error}\\
\text{E}[y_0 - \hat{f}(x_0)]&=\text{Var}(\hat f(x_0))+ [\text{Bias}(\hat f(x_0))]^2 + \text{Var}(\epsilon)
\end{align*}

\subsubsection*{2.2.3: The Classification Setting}\label{the-classification-setting}

\paragraph{The Bayes Classifier}


The \textbf{Bayes classifier} (sometimes referred to as the Bayes optimal classifier) is simply one which assigns to each \(x\) value the class that is most likely, given that \(X=x\). That is, whatever class \(j\) for which 
\begin{equation*}
\text{Pr}(Y=j | X=x)
\end{equation*}
is the largest. A mathematical way of writing this
could be: 
\begin{equation*} 
C(x) = j \text{ if } p_j(x) = \max\{p_1(x), p_2(x), \ldots, p_k(x)\}
\end{equation*}

\begin{aside}
    This can also be represented with a mathematical operation    argmax.  argmax of \(f(x)\) is equal to the \(x\) value
    for which \(f(x)\) is biggest. For a formal definition of argmax,
    please see \href{https://en.wikipedia.org/wiki/Arg_max}{wikipedia}.\\
    Using argmax, we say
    \begin{equation*}
        C_{\text{Bayes}} (x) = \underset{j}{\operatorname{argmax}} \text{Pr}(Y=j | X=x)
    \end{equation*}\\
    If you are not familiar with argmax, we can think of \(\underset{x}{\operatorname{argmax}} f(x)\) as being equal to whatever value
    of \(x\) maximized \(f(x)\).
\end{aside}

The Bayes classifier can be thought of as the optimal classifier, since it will minimize error as it always 
classifies to the most likely class. Unfortunately, we don't know the conditional probability of \(Y\) given
\(x\), but many methods attempt to estimate this conditional distribution. The most basic one is\ldots{}

\paragraph{K-Nearest Neighbors}
For any test observation, \(x_0\), the K-Nearest Neighbors (KNN)
classifier first finds the \(K\) points in the training data closest to
\(x_0\). It then estimates the conditional probability for class \(j\),
given \(X=x_0\) as the percent of points in these \(K\) points with
class \(j\). It then classifies to whichever class has the highest
probability.

Using less math lingo, KNN classifies a point \(x_0\) as whichever class
is most prevalent (in the training data) of the \(K\) points closest to
\(x_0\).

Note: While it is simple, for many problems, K-nearest neighbors works
great!

\section*{Key Takeaways}\label{key-takeaways}

\begin{itemize}
\tightlist
\item  The goal of statistical learning is to estimate the function \(f\)  that connects input variable \(X\) to output variable \(Y\)
\item  There are trade-offs between:

  \begin{itemize}
  \tightlist
  \item   bias and variance
  \item   interpretability and flexibility
  \end{itemize}
\item  Supervised learning can be either \textbf{regression} (quantitative  \(Y\)) or \textbf{classification} (qualitative \(Y\))
\item  Model assessment (testing) should be on data that is reserved for  testing, not what was used to train
\end{itemize}

\end{document}